{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aa73ae-75fd-4c20-b1eb-2bab9443fedb",
   "metadata": {},
   "source": [
    "## PINN#2 Basic Working "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab319557-a838-4ff3-b3fb-06fce97729d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcddb073-e93d-4313-b29a-0e7c8ac2c6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, num_inputs=1, num_layers=1, num_neurons=5, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "        \n",
    "        # Hidden layers with linear layer and activation function\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(act)\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "        \n",
    "        # Build network \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bfcc6-5418-4873-ae55-93894650dbc2",
   "metadata": {},
   "source": [
    "### Build Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b84bbd84-82c2-4364-8212-27c38dbbb857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearNN()\n",
    "x = torch.Tensor\n",
    "params = dict[str, torch.nn.Parameter]\n",
    "\n",
    "def f(x, params) -> torch.Tensor:\n",
    "    return model(x)\n",
    "\n",
    "# Compute the first derivative of f with respect to x\n",
    "def dfdx(x, params) -> torch.Tensor:\n",
    "    # Compute gradients\n",
    "    gradients = torch.autograd.grad(f(x, params), x, create_graph=True)[0]\n",
    "    return gradients\n",
    "\n",
    "# Compute the second derivative of f with respect to x\n",
    "def d2fdx2(x, params) -> torch.Tensor:\n",
    "    # Compute gradients of gradients\n",
    "    gradients = torch.autograd.grad(dfdx(x, params), x, create_graph=True)[0]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e1752-335a-4808-a9ad-d4f02d73917e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the functions defined above, the MSE loss is easily computed as a sum of the DE contribution at each colocation point and the boundary contribution. Notice that, given the functional nature of the forward pass and derivatives, the loss function must also take the model params as input argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20cd09d-c463-4713-b0f6-f1d44fa76810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = 1.0 # rate of maximum population growth parameterizing the equation\n",
    "X_BOUNDRY = 0.0 # boundary condition coordinate\n",
    "F_BOUNDRY = 0.5 # boundary condition value\n",
    "\n",
    "def loss_fn(params:torch.Tensor, x:torch.Tensor):\n",
    "    \n",
    "    # interior loss\n",
    "    f_value = f(x, params)\n",
    "    interior = dfdx(x,params) - R * f_value * ( 1- f_value)\n",
    "    \n",
    "    #boundry loss\n",
    "    x0 = X_BOUNDRY\n",
    "    f0 = F_BOUNDRY\n",
    "    x_boundry = torch.Tensor([x0])\n",
    "    f_boundry = torch.Tensor([f0])\n",
    "    boundry = f(x_boundry, params) -  f_boundry\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    loss_value =  loss(interior, torch.zeros_like(interior)) + loss(\n",
    "        boundary, torch.zeros_like(boundary)\n",
    "    )\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35f9bdb-f0fa-4e80-b14e-33b7c0a601d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchopt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m domain \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m)  \u001b[38;5;66;03m# ;ogistic equation domain\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# choose optimizer with functional API using functorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torchopt\u001b[38;5;241m.\u001b[39mFuncOptimizer(torchopt\u001b[38;5;241m.\u001b[39madam(lr\u001b[38;5;241m=\u001b[39mlearning_rate))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# initial parameters randomly initialized\u001b[39;00m\n\u001b[1;32m     11\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchopt' is not defined"
     ]
    }
   ],
   "source": [
    "# choose the configuration for the training loop\n",
    "batch_size = 30  # number of colocation points to sample in the domain\n",
    "num_iter = 100  # maximum number of iterations\n",
    "learning_rate = 1e-1  # learning rate\n",
    "domain = (-5.0, 5.0)  # ;ogistic equation domain\n",
    "\n",
    "# choose optimizer with functional API using functorch\n",
    "optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "# initial parameters randomly initialized\n",
    "params = tuple(model.parameters())\n",
    "\n",
    "# train the model\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # sample points in the domain randomly for each epoch\n",
    "    x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "    # compute the loss with the current parameters\n",
    "    loss = loss_fn(params, x)\n",
    "\n",
    "    # update the parameters with functional optimizer\n",
    "    params = optimizer.step(loss, params)\n",
    "\n",
    "    print(f\"Iteration {i} with loss {float(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff5752-cc4c-4393-9f8c-018f01b44bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
